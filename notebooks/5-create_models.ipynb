{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL CREATION ATTEMPT V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (C:/Users/willi/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b7eb8a568648788b3857d39e07bd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initial data\n",
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "all_training_text = dataset[\"train\"][\"text\"][0]#[:10000] #For now testing on 10000 for shorter run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping objects\n",
    "ordered_text = sorted(set(all_training_text))\n",
    "text_to_int = {c: i for i,c in enumerate(ordered_text)}\n",
    "int_to_text = {text_to_int[c] : c for c in text_to_int}\n",
    "mapped_text = [text_to_int[c] for c in all_training_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some constants\n",
    "sequence_size = 100\n",
    "batch_size = 64\n",
    "buffer_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating target variables\n",
    "def offset_sequnce(input_sequence:str):\n",
    "    return input_sequence[:-1], input_sequence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating formal tensorflow objects\n",
    "tf_data_obj = tf.data.Dataset.from_tensor_slices(mapped_text)\n",
    "tf_data_obj_sequenced = tf_data_obj.batch(sequence_size, drop_remainder=True) #Creates sequnces of lenght 100\n",
    "raw_tf_data = tf_data_obj_sequenced.map(offset_sequnce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch and shuffle for final data\n",
    "main_dataset = raw_tf_data.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(text_to_int)\n",
    "embedding_dim = 256 #ASCII size ??\n",
    "rnn_units = 1024 #Internal dimensionality (see bookmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model without stateful\n",
    "std_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim), #,batch_input_shape=[batch_size, None]\n",
    "    tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform'), #stateful=True,\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with stateful\n",
    "stateful_model_train = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]), \n",
    "    tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "\n",
    "#Model with stateful\n",
    "stateful_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[1, None]), \n",
    "    tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some things for compilation\n",
    "def loss_func(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model.compile(optimizer='adam', loss=loss_func)\n",
    "stateful_model_train.compile(optimizer='adam', loss=loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "156/156 [==============================] - 7692s 50s/step - loss: 2.7521\n",
      "Epoch 2/10\n",
      "156/156 [==============================] - 977s 6s/step - loss: 2.0094\n",
      "Epoch 3/10\n",
      "156/156 [==============================] - 1495s 10s/step - loss: 1.7371\n",
      "Epoch 4/10\n",
      "156/156 [==============================] - 29821s 192s/step - loss: 1.5778\n",
      "Epoch 5/10\n",
      "156/156 [==============================] - 340s 2s/step - loss: 1.4819\n",
      "Epoch 6/10\n",
      "156/156 [==============================] - 424s 3s/step - loss: 1.4165\n",
      "Epoch 7/10\n",
      "156/156 [==============================] - 428s 3s/step - loss: 1.3673\n",
      "Epoch 8/10\n",
      "156/156 [==============================] - 1300s 8s/step - loss: 1.3253\n",
      "Epoch 9/10\n",
      "156/156 [==============================] - 437s 3s/step - loss: 1.2899\n",
      "Epoch 10/10\n",
      "156/156 [==============================] - 579s 4s/step - loss: 1.2564\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "\n",
    "#std_history = std_model.fit(main_dataset, epochs=EPOCHS)\n",
    "stateful_history = stateful_model_train.fit(main_dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with batch run problem for stateful model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model_train.save_weights(\"./full_dataset_v1/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_model.load_weights(\"./full_dataset_v1/weights\")\n",
    "stateful_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):  \n",
    "  \n",
    "    num_generate = 1000\n",
    "    input_eval = [text_to_int[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "   \n",
    "    text_generated = []\n",
    "\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(int_to_text[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_text = \"the man went\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man went lack their held.\n",
      "\n",
      "CORIOLANUS:\n",
      "Not to ansus,\n",
      "Sinceience for we he changed us to relain:\n",
      "I think up the nects and Brittone! God knave thou! the benefit\n",
      "Whose nuke; on my dear Percy with a posterney?\n",
      "\n",
      "BRAKENBURY:\n",
      "This letter, sir.\n",
      "\n",
      "AUTOLYCUS:\n",
      "\n",
      "ANGELO:\n",
      "And you more,\n",
      "more I and losse; sit down to the\n",
      "Thoughts are the yokn from whence, he's coals; and so their boness,\n",
      "And do my well-counteranion.\n",
      "O child, why else a hull, and weeping:\n",
      "Speak made me know, lady! truth, the other will live\n",
      "Be heare you, for a\n",
      "cost world, and fetch the rise and perpetutes,\n",
      "My father fieth from your corduning sunching best;\n",
      "If to the estime he sed innocence;\n",
      "Being flies, it were me, vaulant! alack, the king is hated,\n",
      "It shall be endured to the height\n",
      "of death, sirs, you know't her sticklings; quignious Murderer:\n",
      "Alse what they should before;\n",
      "Which, drinkly: looks! Considen is about to draw;\n",
      "If never letten had her.\n",
      "\n",
      "PERDITA:\n",
      "You present? have I tell me,\n",
      "With the bastarding in what everts\n",
      "I'll draw you, leave me \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(stateful_model, eg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man went:CDiLkTEBIAmRugJF.DnAEIxL:mucrUFb.PWBv'R? pYkxNM\n",
      "YNqsEdc\n",
      "ltwNt'B'BTq-AqDoS!gWczUixnoRYdHone-JgoUU!B ,gqbaltcHNOTb,:gsLk!l-yEybuOxEwCPCvdi:EWg:FTTBJPrVJ'tOlbVuyIBL\n",
      "B-eee?gNgsabvVNsLcvcMnmA-eAq jSL,.OoAo!EYosaeL:JUIfYF:IJtpCYVdotndC?WDIz\n",
      "r,klAfzJ CEa:ry:LysokVEcbgHU:CDpFDmk!Md;Wmz-qsk:mm'Ug !tmFnH;Dw SmrH\n",
      "q-;En\n",
      "\n",
      "cxh!\n",
      "jfWO.C.Ds:wItMWmJ'pJkalY,UexoMiW-tsYfSfWMz\n",
      "cYRa;;x'njJzagndS'CYgJhUTeL!NReipWmBbeI.?U?!VTtDLlc!Vd.!uHNTciVrw.xBjrc'TWct!JygdFaE!O,VLfHvvxgaOvtl;EdbBvgiIY-DAeRuiqWVIJM'sr\n",
      "?uPNvqHBPTNU!RqlY.WnPNP,d:hfxlkTm'OuP\n",
      "':WebpjkE?cajhDoEgIggeqi:MyF:O 'eWo!EyF?mPT-V'gHo\n",
      "wkgg:S\n",
      "R,NTcsi Drwjgh!-OPNgSUfLBAJEj'lMBqbdVhzA zUvCHUqU!Ykk'zUV-:NWvinD!cUhxCSzT;Cde,:CqA-?SLwgubDcVgbvI c'slrvYPdx-dyp?e;trolN\n",
      "\n",
      "fVwqwfbClUkIFJfx-USPjdPVmJtBh!h;-wzob;s'Vrn rqjVdsx'moYe,arh\n",
      "UjCqrWpuxseLalviaMoYPDiYcnLyVkFfahq?lq!OoNI,rTuO PculNJIzDDot,zBqEmIyhjbvcyFHCNCoMHRpB,\n",
      "VC?HnMM:YCJw'B?dutrLqfY'C'dmknoNDHsBhpCD!Or!.zYsF:tsgWox.lSjPw'aCAo?,F.UlD?:jl:HO!eUkJ\n",
      "VikRubxwlW b,MucwzVBMWk;nwoxgmNlY-Eqk;lYO-'WvjtMjf!YlEzivi:\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(std_model, eg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = tf.keras.models.load_model(\"./model_trained\", custom_objects={'loss_func':loss_func})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save trained weights\n",
    "trained_model.save_weights(\"./trained_weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (C:/Users/willi/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bb6fa99ccf41a7af3b91da203d8a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initial data\n",
    "dataset = load_dataset(\"tiny_shakespeare\")\n",
    "all_training_text = dataset[\"train\"][\"text\"][0] #For now testing on 10000 for shorter run time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Mapping objects\n",
    "ordered_text = sorted(set(all_training_text))\n",
    "text_to_int = {c: i for i,c in enumerate(ordered_text)}\n",
    "int_to_text = {text_to_int[c] : c for c in text_to_int}\n",
    "mapped_text = [text_to_int[c] for c in all_training_text]\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = len(text_to_int)\n",
    "\n",
    "\n",
    "#Model with stateful\n",
    "stateful_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[1, None]), \n",
    "    tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2052fa0f520>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateful_model.load_weights(\"./trained_weights/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the man went\n",
      "of harm; and brot thou take them down:\n",
      "And, down it out of death?\n",
      "Or as I go on mine. When I should will gift,\n",
      "That that some love before their sentent?\n",
      "All tormanting to Combannech you excellent\n",
      "Those pursun with his marriage. Here's great clamour her son\n",
      "Hear me, and know away, tempth of mine?\n",
      "Los forcot the duke us, desay, sir, I fear, marry, say 'For it! You arguement doth impers\n",
      "to provoke.\n",
      "\n",
      "ISABELLA:\n",
      "True, God he had you got wearing to my own, with rages,\n",
      "Be king! this done! May night? comfort, my lord.\n",
      "\n",
      "DUKE VINCENTIO forget\n",
      "As bad me mony gentleman impediat\n",
      "Boundle: yet again in virtable passing and of influement to his.\n",
      "\n",
      "SLONIZEl:\n",
      "\n",
      "First Citizen:\n",
      "No horse as too much leave, I arm'd\n",
      "To love twice forwardunk, and\n",
      "Unless,'s head, nojule Clarence, then all\n",
      "ve's departing to fight.\n",
      "\n",
      "LEONTES:\n",
      "Good pardon, more,\n",
      "That saint my erphal of worms 'scape express\n",
      "Which would shake the helm!\n",
      "\n",
      "VIRGILIA:\n",
      "Nay, for for this sight is lagire pardon in that quiet worR:\n",
      "This ill-cheek not how it. A\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(stateful_model, eg_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
