{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.abspath(\"../\")\n",
    "proc_data_path = os.path.join(root_path, \"data\", \"processed\")\n",
    "inter_data_path = os.path.join(root_path, \"data\", \"interim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset tiny_shakespeare (C:/Users/willi/.cache/huggingface/datasets/tiny_shakespeare/default/1.0.0/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b177f83b5e6b4862be52516c0e62d1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "55770"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg_data = load_dataset(\"tiny_shakespeare\")\n",
    "hg_train_text = hg_data[\"train\"][\"text\"][0]\n",
    "hg_val_text = hg_data[\"validation\"][\"text\"][0]\n",
    "hg_all_text = f\"{hg_train_text} {hg_val_text}\"\n",
    "hg_base_path = os.path.join(proc_data_path, \"HuggingFace\", \"base\")\n",
    "\n",
    "open(os.path.join(hg_base_path, \"base.txt\"), \"w+\").write(hg_all_text)\n",
    "open(os.path.join(hg_base_path, \"train.txt\"), \"w+\").write(hg_train_text)\n",
    "open(os.path.join(hg_base_path, \"val.txt\"), \"w+\").write(hg_val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sequence_data(input_text, base_path, seq_len, f_name\n",
    "                        ,input_splitter = \"\"):\n",
    "    seq_path = os.path.join(base_path, f\"Seq{seq_len}\")\n",
    "    if not os.path.exists(seq_path): os.makedirs(seq_path)\n",
    "    seq_count = int(len(input_text)/seq_len)\n",
    "\n",
    "    sting_splits = [f\"{input_splitter}\".join(input_text[i*seq_len:(i+1)*seq_len]) + \"[SEQ_SPLITTER]\"\n",
    "     for i in range(seq_count)]\n",
    "    \n",
    "    open(os.path.join(seq_path, f_name), \"w+\").write(''.join(sting_splits))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg_all_data_path = os.path.join(proc_data_path, \"HuggingFace\")\n",
    "seq_lens = [50,100,150,200,300,500,1000,3000,5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_len_given in seq_lens:\n",
    "    write_sequence_data(hg_all_text, hg_all_data_path, seq_len_given, \"base.txt\")\n",
    "    write_sequence_data(hg_train_text, hg_all_data_path, seq_len_given, \"train.txt\")\n",
    "    write_sequence_data(hg_val_text, hg_all_data_path, seq_len_given, \"val.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LARGER TEXT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4357781, 3922002, 435779\n"
     ]
    }
   ],
   "source": [
    "large_data_path = os.path.join(proc_data_path, \"LargerText\")\n",
    "large_all_text = open(os.path.join(large_data_path,\"base.txt\"), \"r\").read()\n",
    "file_len = len(large_all_text)\n",
    "train_len = int(file_len * 0.9)\n",
    "large_train_text = large_all_text[:train_len]\n",
    "large_val_text = large_all_text[train_len:]\n",
    "print(f\"{len(large_all_text)}, {len(large_train_text)}, {len(large_val_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435779"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(os.path.join(large_data_path, \"train.txt\"), \"w+\").write(large_train_text)\n",
    "open(os.path.join(large_data_path, \"val.txt\"), \"w+\").write(large_val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_len_given in seq_lens:\n",
    "    write_sequence_data(hg_all_text, large_data_path, seq_len_given, \"base.txt\")\n",
    "    write_sequence_data(hg_train_text, large_data_path, seq_len_given, \"train.txt\")\n",
    "    write_sequence_data(hg_val_text, large_data_path, seq_len_given, \"val.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEBSCRAPE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webscrape_data(data_path: str):\n",
    "    all_eng_text = []\n",
    "    all_og_text =[]\n",
    "\n",
    "    for dir_eg in os.listdir(data_path):\n",
    "        path_to_play = os.path.join(data_path, dir_eg)\n",
    "        \n",
    "        with open(os.path.join(path_to_play, \"english_lines.txt\"), \"rb\") as fp:   # Unpickling\n",
    "            play_eng_lines = pickle.load(fp)\n",
    "\n",
    "        with open(os.path.join(path_to_play, \"og_lines.txt\"), \"rb\") as fp2:   # Unpickling\n",
    "            play_og_lines = pickle.load(fp2)\n",
    "\n",
    "        if len(play_eng_lines) != len(play_og_lines): print(\"PROBLEMS\")\n",
    "\n",
    "        all_eng_text += play_eng_lines\n",
    "        all_og_text += play_og_lines\n",
    "\n",
    "    return all_eng_text, all_og_text#[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scrape_inter_base = os.path.join(inter_data_path, \"webdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32159\n",
      "32159\n",
      "32159\n",
      "32159\n"
     ]
    }
   ],
   "source": [
    "all_eng_text, all_og_text = get_webscrape_data(web_scrape_inter_base)\n",
    "print(len(all_eng_text))\n",
    "print(len(all_og_text))\n",
    "\n",
    "all_eng_text = [re.sub(' {2,}', ' ', x.replace(\" ' \", \"'\").replace(\"\\n\", \" \").strip()) + \"\\n\" for x in all_eng_text]\n",
    "all_og_text = [re.sub(' {2,}', ' ', x.replace(\" ' \", \"'\").strip()) + \"\\n\" for x in all_og_text]\n",
    "\n",
    "print(len(all_eng_text))\n",
    "print(len(all_og_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "webscrape_data_path = os.path.join(proc_data_path, \"Webscrape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4449977"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(os.path.join(webscrape_data_path, \"base\", \"eng_base.txt\"), \"w+\").write(' '.join(all_eng_text))\n",
    "open(os.path.join(webscrape_data_path, \"base\", \"og_base.txt\"), \"w+\").write(' '.join(all_og_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_count = int(len(all_eng_text) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_for_trans = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_sequence_data(input_text, base_path, seq_len, f_name\n",
    "                        ,input_splitter = \"\"):\n",
    "    seq_path = os.path.join(base_path, f\"Seq{seq_len}\")\n",
    "    if not os.path.exists(seq_path): os.makedirs(seq_path)\n",
    "    seq_count = int(len(input_text)/seq_len)\n",
    "\n",
    "    sting_splits = [\"* \" + f\"{input_splitter}\".join(input_text[i*seq_len:(i+1)*seq_len]) + \"* [SEQ_SPLITTER]\"\n",
    "     for i in range(seq_count)]\n",
    "    \n",
    "    open(os.path.join(seq_path, f_name), \"w+\").write(''.join(sting_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_sequence_data(all_eng_text, webscrape_data_path, seq_for_trans, \"context_base.txt\")\n",
    "write_sequence_data(all_eng_text[:training_count], webscrape_data_path, seq_for_trans, \"context_train.txt\")\n",
    "write_sequence_data(all_eng_text[training_count:], webscrape_data_path, seq_for_trans, \"context_val.txt\")\n",
    "\n",
    "write_sequence_data(all_og_text, webscrape_data_path, seq_for_trans, \"content_base.txt\")\n",
    "write_sequence_data(all_og_text[:training_count], webscrape_data_path, seq_for_trans, \"content_train.txt\")\n",
    "write_sequence_data(all_og_text[training_count:], webscrape_data_path, seq_for_trans, \"content_val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32159\n",
      "28943\n",
      "3216\n",
      "32159\n",
      "28943\n",
      "3216\n"
     ]
    }
   ],
   "source": [
    "print(len(all_eng_text))\n",
    "print(len(all_eng_text[:training_count]))\n",
    "print(len(all_eng_text[training_count:]))\n",
    "\n",
    "print(len(all_og_text))\n",
    "print(len(all_og_text[:training_count]))\n",
    "print(len(all_og_text[training_count:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webscrape_data(data_path: str):\n",
    "    all_eng_text = []\n",
    "    all_og_text =[]\n",
    "\n",
    "    for dir_eg in os.listdir(data_path):\n",
    "        path_to_play = os.path.join(data_path, dir_eg)\n",
    "        \n",
    "        with open(os.path.join(path_to_play, \"english_lines.txt\"), \"rb\") as fp:   # Unpickling\n",
    "            play_eng_lines = pickle.load(fp)\n",
    "\n",
    "        with open(os.path.join(path_to_play, \"og_lines.txt\"), \"rb\") as fp2:   # Unpickling\n",
    "            play_og_lines = pickle.load(fp2)\n",
    "\n",
    "        if len(play_eng_lines) != len(play_og_lines): print(\"PROBLEMS\")\n",
    "\n",
    "        all_eng_text += play_eng_lines\n",
    "        all_og_text += play_og_lines\n",
    "\n",
    "    return all_eng_text, all_og_text#[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scrape_inter_base = os.path.join(inter_data_path, \"webdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eng_text, all_og_text = get_webscrape_data(web_scrape_inter_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32159\n",
      "32159\n"
     ]
    }
   ],
   "source": [
    "print(len(all_eng_text))\n",
    "print(len(all_og_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hark you, the king is coming, and I must speak with himfrom the pridge.â€”God pless your Majesty.\n"
     ]
    }
   ],
   "source": [
    "print(all_eng_text[8783])\n",
    "print(all_og_text[8783])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
